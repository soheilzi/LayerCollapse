{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchprofile import profile_macs\n",
    "\n",
    "assert torch.cuda.is_available(), \\\n",
    "\"CUDA support is not available.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9780528d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_url(url, model_dir='.', overwrite=False):\n",
    "#     import os, sys\n",
    "#     from urllib.request import urlretrieve\n",
    "#     target_dir = url.split('/')[-1]\n",
    "#     model_dir = os.path.expanduser(model_dir)\n",
    "#     try:\n",
    "#         if not os.path.exists(model_dir):\n",
    "#             os.makedirs(model_dir)\n",
    "#         model_dir = os.path.join(model_dir, target_dir)\n",
    "#         cached_file = model_dir\n",
    "#         if not os.path.exists(cached_file) or overwrite:\n",
    "#             sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "#             urlretrieve(url, cached_file)\n",
    "#         return cached_file\n",
    "#     except Exception as e:\n",
    "#         # remove lock file so download can be executed next time.\n",
    "#         os.remove(os.path.join(model_dir, 'download.lock'))\n",
    "#         sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGG(nn.Module):\n",
    "#   ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "#   def __init__(self) -> None:\n",
    "#     super().__init__()\n",
    "\n",
    "#     layers = []\n",
    "#     counts = defaultdict(int)\n",
    "\n",
    "#     def add(name: str, layer: nn.Module) -> None:\n",
    "#       layers.append((f\"{name}{counts[name]}\", layer))\n",
    "#       counts[name] += 1\n",
    "\n",
    "#     last_layer_negative_slope = nn.Parameter(torch.tensor(0.5), requires_grad=False)\n",
    "#     in_channels = 3\n",
    "#     i = 0\n",
    "#     for x in self.ARCH:\n",
    "#       if x != 'M':\n",
    "#         # conv-bn-relu\n",
    "#         add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "#         add(\"bn\", nn.BatchNorm2d(x))\n",
    "#         if i == 9 or i == 6:\n",
    "#           add(\"reluleaky\", nn.LeakyReLU(negative_slope=last_layer_negative_slope, inplace=True))\n",
    "#         else:\n",
    "#           add(\"relu\", nn.ReLU(inplace=True))\n",
    "#         in_channels = x\n",
    "#       else:\n",
    "#         # maxpool\n",
    "#         add(\"pool\", nn.MaxPool2d(2))\n",
    "#       i += 1\n",
    "\n",
    "#     self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "#     self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "#   def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#     # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "#     x = self.backbone(x)\n",
    "\n",
    "#     # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "#     x = x.mean([2, 3])\n",
    "\n",
    "#     # classifier: [N, 512] => [N, 10]\n",
    "#     x = self.classifier(x)\n",
    "#     return x\n",
    "  \n",
    "#   @torch.no_grad()\n",
    "#   def load_my_state_dict(self, state_dict):\n",
    " \n",
    "#         own_state = self.state_dict()\n",
    "#         for name, param in state_dict.items():\n",
    "#             if name not in own_state:\n",
    "#                  continue\n",
    "#             if isinstance(param, nn.Parameter):\n",
    "#                 # backwards compatibility for serialized parameters\n",
    "#                 param = param.data\n",
    "#             own_state[name].copy_(param)\n",
    "\n",
    "#   @torch.no_grad()\n",
    "#   def change_negative_slope(self, negative_slope: float) -> None:\n",
    "#     for m in self.modules():\n",
    "#       if isinstance(m, nn.LeakyReLU):\n",
    "#         m.negative_slope = nn.Parameter(torch.tensor(negative_slope), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FinetuneClassifier(nn.Module):\n",
    "#     def __init__(self, backbone: nn.Module, num_classes: int, num_layers, layer_size, input_dim) -> None:\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.layers = [(f'classifier_linear{0}', nn.Linear(input_dim, layer_size))]\n",
    "#         self.layers.append((f'LeakyReLU{0}', nn.LeakyReLU(negative_slope=nn.Parameter(torch.tensor(0.01), requires_grad=False), inplace=True)))\n",
    "#         self.num_layers = num_layers\n",
    "#         self.layer_size = layer_size\n",
    "#         for i in range(1, num_layers-1):\n",
    "#             self.layers.append((f'classifier_linear{i}', nn.Linear(layer_size, layer_size)))\n",
    "#             negative_slope = nn.Parameter(torch.tensor(0.01), requires_grad=False)\n",
    "#             self.layers.append((f'LeakyReLu{i}', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)))\n",
    "\n",
    "#         self.final_layer = nn.Linear(layer_size, num_classes)\n",
    "\n",
    "#         self.classifier = nn.Sequential(OrderedDict(self.layers))\n",
    "\n",
    "#         self.model = nn.Sequential(OrderedDict([('backbone', self.backbone), ('classifier', self.classifier), ('final_layer', self.final_layer)]))\n",
    "    \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.model(x)\n",
    "#         return x\n",
    "    \n",
    "#     @torch.no_grad()\n",
    "#     def change_negative_slope(self, negative_slope: float, layer) -> None:\n",
    "#         for name, m in self.named_modules():\n",
    "#             if name == 'LeakyReLU0':\n",
    "#                 m.negative_slope = nn.Parameter(torch.tensor(negative_slope), requires_grad=False)\n",
    "#             elif name == 'LeakyReLU1':\n",
    "#                 m.negative_slope = nn.Parameter(torch.tensor(negative_slope), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  criterion: nn.Module,\n",
    "  optimizer: Optimizer,\n",
    "  scheduler: LambdaLR,\n",
    "  callbacks = None\n",
    ") -> None:\n",
    "  model.train()\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Reset the gradients (from the last iteration)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward inference\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update optimizer and LR scheduler\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader, \n",
    "  verbose=True,\n",
    ") -> float:\n",
    "  # model.eval()\n",
    "\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False, \n",
    "                              disable=not verbose):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)\n",
    "\n",
    "\n",
    "def get_sparsity(tensor: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    calculate the sparsity of the given tensor\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    \"\"\"\n",
    "    return 1 - float(tensor.count_nonzero()) / tensor.numel()\n",
    "\n",
    "\n",
    "def get_model_sparsity(model: nn.Module) -> float:\n",
    "    \"\"\"\n",
    "    calculate the sparsity of the given model\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    \"\"\"\n",
    "    num_nonzeros, num_elements = 0, 0\n",
    "    for param in model.parameters():\n",
    "        num_nonzeros += param.count_nonzero()\n",
    "        num_elements += param.numel()\n",
    "    return 1 - float(num_nonzeros) / num_elements\n",
    "\n",
    "def get_num_parameters(model: nn.Module, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the total number of parameters of model\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    num_counted_elements = 0\n",
    "    for param in model.parameters():\n",
    "        if count_nonzero_only:\n",
    "            num_counted_elements += param.count_nonzero()\n",
    "        else:\n",
    "            num_counted_elements += param.numel()\n",
    "    return num_counted_elements\n",
    "\n",
    "\n",
    "def get_model_size(model: nn.Module, data_width=32, count_nonzero_only=False) -> int:\n",
    "    \"\"\"\n",
    "    calculate the model size in bits\n",
    "    :param data_width: #bits per element\n",
    "    :param count_nonzero_only: only count nonzero weights\n",
    "    \"\"\"\n",
    "    return get_num_parameters(model, count_nonzero_only) * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_url = \"https://hanlab.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
    "# checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
    "# print(f\"=> loading checkpoint '{checkpoint_url}'\")\n",
    "# model = VGG().cuda()\n",
    "# model.load_my_state_dict(checkpoint['state_dict'])\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"backbone.relu6.negative_slope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "    ]),\n",
    "    \"test\": ToTensor(),\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.change_negative_slope(0.0)\n",
    "# model.cpu()\n",
    "# freeze model\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuneModel = FinetuneClassifier(model, num_classes=10, num_layers=3, layer_size=512, input_dim=1000).cuda()\n",
    "finetuneModel = nn.Sequential(OrderedDict([('backbone', model), ('classifier', nn.Linear(1000, 10))])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning Fine-grained Pruned Sparse Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1 Accuracy 59.70% / Best Accuracy: 59.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2 Accuracy 75.24% / Best Accuracy: 75.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3 Accuracy 78.73% / Best Accuracy: 78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4 Accuracy 81.82% / Best Accuracy: 81.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5 Accuracy 82.26% / Best Accuracy: 82.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 6 Accuracy 84.88% / Best Accuracy: 84.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7 Accuracy 84.82% / Best Accuracy: 84.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 8 Accuracy 85.47% / Best Accuracy: 85.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 9 Accuracy 86.28% / Best Accuracy: 86.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10 Accuracy 86.43% / Best Accuracy: 86.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 11 Accuracy 87.66% / Best Accuracy: 87.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 12 Accuracy 87.65% / Best Accuracy: 87.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 13 Accuracy 88.21% / Best Accuracy: 88.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 14 Accuracy 88.45% / Best Accuracy: 88.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 15 Accuracy 88.54% / Best Accuracy: 88.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 16 Accuracy 89.00% / Best Accuracy: 89.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 17 Accuracy 89.36% / Best Accuracy: 89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 18 Accuracy 89.33% / Best Accuracy: 89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 19 Accuracy 89.32% / Best Accuracy: 89.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 20 Accuracy 89.44% / Best Accuracy: 89.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 21 Accuracy 89.70% / Best Accuracy: 89.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 22 Accuracy 89.69% / Best Accuracy: 89.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 23 Accuracy 89.75% / Best Accuracy: 89.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 24 Accuracy 89.83% / Best Accuracy: 89.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 25 Accuracy 89.86% / Best Accuracy: 89.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 26 Accuracy 89.83% / Best Accuracy: 89.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 27 Accuracy 90.46% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 28 Accuracy 90.17% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 29 Accuracy 89.77% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 30 Accuracy 90.19% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 31 Accuracy 89.88% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 32 Accuracy 90.13% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 33 Accuracy 89.89% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 34 Accuracy 90.02% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 35 Accuracy 89.59% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 36 Accuracy 89.28% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 37 Accuracy 89.71% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 38 Accuracy 89.16% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 39 Accuracy 89.62% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 40 Accuracy 89.42% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 41 Accuracy 89.84% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 42 Accuracy 89.27% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 43 Accuracy 89.39% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 44 Accuracy 89.27% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 45 Accuracy 89.61% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 46 Accuracy 90.03% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 47 Accuracy 89.19% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 48 Accuracy 90.08% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 49 Accuracy 89.77% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 50 Accuracy 89.33% / Best Accuracy: 90.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 50\n",
    "optimizer = torch.optim.SGD(finetuneModel.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_sparse_model_checkpoint = dict()\n",
    "best_accuracy = 0\n",
    "print(f'Finetuning Fine-grained Pruned Sparse Model')\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    # At the end of each train iteration, we have to apply the pruning mask \n",
    "    #    to keep the model sparse during the training\n",
    "    train(finetuneModel, dataloader['train'], criterion, optimizer, scheduler,\n",
    "          callbacks=None)\n",
    "    accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "    is_best = accuracy > best_accuracy\n",
    "    if is_best:\n",
    "        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(finetuneModel.state_dict())\n",
    "        best_accuracy = accuracy\n",
    "    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=89.34%\n",
      "dense model has size=527.83 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model paremeters\n",
    "torch.save(best_sparse_model_checkpoint, 'best_sparse_model_checkpoint.pth')\n",
    "# lod sparse model parameters\n",
    "finetuneModel.load_state_dict(best_sparse_model_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "got it\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '4' and isinstance(m3, nn.LeakyReLU):\n",
    "                        print(\"got it\")\n",
    "                        m2._modules[name3] = nn.LeakyReLU(negative_slope=nn.Parameter(torch.tensor(0.5), requires_grad=False), inplace=True)\n",
    "                        # setattr(m3, 'Relu', nn.LeakyReLU(negative_slope=nn.Parameter(torch.tensor(0.1), requires_grad=False), inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): LeakyReLU(\n",
      "        negative_slope=Parameter containing:\n",
      "        tensor(0.5000), inplace=True\n",
      "      )\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=74.67%\n",
      "dense model has size=527.83 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning Fine-grained Pruned Sparse Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1 Accuracy 87.72% / Best Accuracy: 87.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2 Accuracy 88.06% / Best Accuracy: 88.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3 Accuracy 85.49% / Best Accuracy: 88.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4 Accuracy 86.87% / Best Accuracy: 88.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5 Accuracy 88.74% / Best Accuracy: 88.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 5\n",
    "optimizer = torch.optim.SGD(finetuneModel.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_sparse_model_checkpoint = dict()\n",
    "best_accuracy = 0\n",
    "print(f'Finetuning Fine-grained Pruned Sparse Model')\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    # At the end of each train iteration, we have to apply the pruning mask \n",
    "    #    to keep the model sparse during the training\n",
    "    train(finetuneModel, dataloader['train'], criterion, optimizer, scheduler,\n",
    "          callbacks=None)\n",
    "    accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "    is_best = accuracy > best_accuracy\n",
    "    if is_best:\n",
    "        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(finetuneModel.state_dict())\n",
    "        best_accuracy = accuracy\n",
    "    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=88.72%\n",
      "dense model has size=527.83 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "got it\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '4' and isinstance(m3, nn.LeakyReLU):\n",
    "                        print(\"got it\")\n",
    "                        m2._modules[name3] = nn.LeakyReLU(negative_slope=nn.Parameter(torch.tensor(1), requires_grad=False), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): LeakyReLU(\n",
      "        negative_slope=Parameter containing:\n",
      "        tensor(1), inplace=True\n",
      "      )\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning Fine-grained Pruned Sparse Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1 Accuracy 90.19% / Best Accuracy: 90.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2 Accuracy 86.05% / Best Accuracy: 90.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3 Accuracy 89.85% / Best Accuracy: 90.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4 Accuracy 89.82% / Best Accuracy: 90.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5 Accuracy 89.38% / Best Accuracy: 90.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 5\n",
    "optimizer = torch.optim.SGD(finetuneModel.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_sparse_model_checkpoint = dict()\n",
    "best_accuracy = 0\n",
    "print(f'Finetuning Fine-grained Pruned Sparse Model')\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    # At the end of each train iteration, we have to apply the pruning mask \n",
    "    #    to keep the model sparse during the training\n",
    "    train(finetuneModel, dataloader['train'], criterion, optimizer, scheduler,\n",
    "          callbacks=None)\n",
    "    accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "    is_best = accuracy > best_accuracy\n",
    "    if is_best:\n",
    "        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(finetuneModel.state_dict())\n",
    "        best_accuracy = accuracy\n",
    "    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=89.22%\n",
      "dense model has size=527.83 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "got it\n",
      "4\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w3 = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '3':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        w3 = m3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w6 = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        w6 = m3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4096])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4096])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w6 = torch.matmul(w6, w3)\n",
    "new_w6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "got it\n",
      "4\n",
      "got it\n",
      "5\n",
      "got it\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '3' or name3 == '4' or name3 == '5':\n",
    "                        print(\"got it\")\n",
    "                        # remover this layer\n",
    "                        m2._modules[name3] = nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        m3.weight.data = new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Identity()\n",
      "      (4): Identity()\n",
      "      (5): Identity()\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=88.89%\n",
      "dense model has size=463.81 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now remove the other relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "got it\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '1' and isinstance(m3, nn.LeakyReLU):\n",
    "                        print(\"got it\")\n",
    "                        m2._modules[name3] = nn.LeakyReLU(negative_slope=nn.Parameter(torch.tensor(1), requires_grad=False), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): LeakyReLU(\n",
      "        negative_slope=Parameter containing:\n",
      "        tensor(1), inplace=True\n",
      "      )\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Identity()\n",
      "      (4): Identity()\n",
      "      (5): Identity()\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=88.54%\n",
      "dense model has size=463.81 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning Fine-grained Pruned Sparse Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1 Accuracy 88.77% / Best Accuracy: 88.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2 Accuracy 90.53% / Best Accuracy: 90.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3 Accuracy 90.61% / Best Accuracy: 90.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4 Accuracy 90.64% / Best Accuracy: 90.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5 Accuracy 90.75% / Best Accuracy: 90.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "num_finetune_epochs = 5\n",
    "optimizer = torch.optim.SGD(finetuneModel.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_sparse_model_checkpoint = dict()\n",
    "best_accuracy = 0\n",
    "print(f'Finetuning Fine-grained Pruned Sparse Model')\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    # At the end of each train iteration, we have to apply the pruning mask \n",
    "    #    to keep the model sparse during the training\n",
    "    train(finetuneModel, dataloader['train'], criterion, optimizer, scheduler,\n",
    "          callbacks=None)\n",
    "    accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "    is_best = accuracy > best_accuracy\n",
    "    if is_best:\n",
    "        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(finetuneModel.state_dict())\n",
    "        best_accuracy = accuracy\n",
    "    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "got it\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w6 = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '0':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        w0 = m3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 25088])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w6 = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        w6 = m3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4096])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w6 = torch.matmul(w6, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "got it\n",
      "1\n",
      "got it\n",
      "2\n",
      "got it\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '0' or name3 == '1' or name3 == '2':\n",
    "                        print(\"got it\")\n",
    "                        # remover this layer\n",
    "                        m2._modules[name3] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        m3.weight.data = new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=90.81%\n",
      "dense model has size=151.88 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (backbone): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Identity()\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "      (3): Identity()\n",
      "      (4): Identity()\n",
      "      (5): Identity()\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(finetuneModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w_classifier = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'classifier':\n",
    "        w_classifier = m.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_classifier.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "w6 = 0\n",
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        #get weights of mateix of liniear\n",
    "                        w6 = m3.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 25088])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classifier = torch.matmul(w_classifier, w6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "features\n",
      "avgpool\n",
      "classifier\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "got it\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'backbone':\n",
    "        for name2, m2 in m.named_children():\n",
    "            print(name2)\n",
    "            if name2 == 'classifier':\n",
    "                for name3, m3 in m2.named_children():\n",
    "                    print(name3)\n",
    "                    if name3 == '6':\n",
    "                        print(\"got it\")\n",
    "                        # remover this layer\n",
    "                        m2._modules[name3] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "for name, m in finetuneModel.named_children():\n",
    "    print(name)\n",
    "    if name == 'classifier':\n",
    "        m.weight.data = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense model has accuracy=90.82%\n",
      "dense model has size=57.09 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dense_model_accuracy = evaluate(finetuneModel, dataloader['test'])\n",
    "dense_model_size = get_model_size(finetuneModel)\n",
    "print(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n",
    "print(f\"dense model has size={dense_model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313472512"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32).cuda()\n",
    "get_model_macs(finetuneModel, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(finetuneModel.state_dict(), 'finetuneModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inference time is 0.0011 seconds\n"
     ]
    }
   ],
   "source": [
    "# test inference time\n",
    "import time\n",
    "finetuneModel.eval()\n",
    "average_time = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        start = time.time()\n",
    "        output = finetuneModel(dummy_input)\n",
    "        end = time.time()\n",
    "        average_time += (end - start)\n",
    "\n",
    "print(f\"average inference time is {average_time/1000:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/soheil/miniconda3/envs/hardwareAcc/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_ = models.vgg16(pretrained=True)\n",
    "old_finetuneModel = nn.Sequential(OrderedDict([('backbone', model_), ('classifier', nn.Linear(1000, 10))])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436865296"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32).cuda()\n",
    "get_model_macs(old_finetuneModel, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inference time is 0.0029 seconds\n"
     ]
    }
   ],
   "source": [
    "old_finetuneModel.eval()\n",
    "average_time = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        start = time.time()\n",
    "        output = old_finetuneModel(dummy_input)\n",
    "        end = time.time()\n",
    "        average_time += (end - start)\n",
    "\n",
    "print(f\"average inference time is {average_time/1000:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardwareAcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
