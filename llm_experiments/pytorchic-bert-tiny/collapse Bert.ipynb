{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b77f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966236d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchprofile import profile_macs\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.distributed as dist\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "\n",
    "assert torch.cuda.is_available(), \\\n",
    "\"CUDA support is not available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122292b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff5e54f2fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# set device \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#set global device\n",
    "# torch.cuda.set_device(device)\n",
    "# dist.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7c368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "import fire\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tokenization\n",
    "import models\n",
    "import optim\n",
    "import train\n",
    "\n",
    "from utils import set_seeds, get_device, truncate_tokens_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154513d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvDataset(Dataset):\n",
    "    \"\"\" Dataset Class for CSV file \"\"\"\n",
    "    labels = None\n",
    "    def __init__(self, file, pipeline=[]): # cvs file and pipeline object\n",
    "        Dataset.__init__(self)\n",
    "        data = []\n",
    "        with open(file, \"r\") as f:\n",
    "            # list of splitted lines : line is also list\n",
    "            lines = csv.reader(f, delimiter='\\t', quotechar=None)\n",
    "            for instance in self.get_instances(lines): # instance : tuple of fields\n",
    "                for proc in pipeline: # a bunch of pre-processing\n",
    "                    instance = proc(instance)\n",
    "                data.append(instance)\n",
    "\n",
    "        # To Tensors\n",
    "        self.tensors = [torch.tensor(x, dtype=torch.long) for x in zip(*data)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def get_instances(self, lines):\n",
    "        \"\"\" get instance array from (csv-separated) line list \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef230ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRPC(CsvDataset):\n",
    "    \"\"\" Dataset class for MRPC \"\"\"\n",
    "    labels = (\"0\", \"1\") # label names\n",
    "    def __init__(self, file, pipeline=[]):\n",
    "        super().__init__(file, pipeline)\n",
    "\n",
    "    def get_instances(self, lines):\n",
    "        for line in itertools.islice(lines, 1, None): # skip header\n",
    "            yield line[0], line[3], line[4] # label, text_a, text_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56184d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNLI(CsvDataset):\n",
    "    \"\"\" Dataset class for MNLI \"\"\"\n",
    "    labels = (\"contradiction\", \"entailment\", \"neutral\") # label names\n",
    "    def __init__(self, file, pipeline=[]):\n",
    "        super().__init__(file, pipeline)\n",
    "\n",
    "    def get_instances(self, lines):\n",
    "        for line in itertools.islice(lines, 1, None): # skip header\n",
    "            yield line[-1], line[8], line[9] # label, text_a, text_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6cd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    \"\"\" Preprocess Pipeline Class : callable \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class Tokenizing(Pipeline):\n",
    "    \"\"\" Tokenizing sentence pair \"\"\"\n",
    "    def __init__(self, preprocessor, tokenize):\n",
    "        super().__init__()\n",
    "        self.preprocessor = preprocessor # e.g. text normalization\n",
    "        self.tokenize = tokenize # tokenize function\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        label, text_a, text_b = instance\n",
    "\n",
    "        label = self.preprocessor(label)\n",
    "        tokens_a = self.tokenize(self.preprocessor(text_a))\n",
    "        tokens_b = self.tokenize(self.preprocessor(text_b)) \\\n",
    "                   if text_b else []\n",
    "\n",
    "        return (label, tokens_a, tokens_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf9365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddSpecialTokensWithTruncation(Pipeline):\n",
    "    \"\"\" Add special tokens [CLS], [SEP] with truncation \"\"\"\n",
    "    def __init__(self, max_len=512):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        label, tokens_a, tokens_b = instance\n",
    "\n",
    "        # -3 special tokens for [CLS] text_a [SEP] text_b [SEP]\n",
    "        # -2 special tokens for [CLS] text_a [SEP]\n",
    "        _max_len = self.max_len - 3 if tokens_b else self.max_len - 2\n",
    "        truncate_tokens_pair(tokens_a, tokens_b, _max_len)\n",
    "\n",
    "        # Add Special Tokens\n",
    "        tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
    "        tokens_b = tokens_b + ['[SEP]'] if tokens_b else []\n",
    "\n",
    "        return (label, tokens_a, tokens_b)\n",
    "\n",
    "\n",
    "class TokenIndexing(Pipeline):\n",
    "    \"\"\" Convert tokens into token indexes and do zero-padding \"\"\"\n",
    "    def __init__(self, indexer, labels, max_len=512):\n",
    "        super().__init__()\n",
    "        self.indexer = indexer # function : tokens to indexes\n",
    "        # map from a label name to a label index\n",
    "        self.label_map = {name: i for i, name in enumerate(labels)}\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, instance):\n",
    "        label, tokens_a, tokens_b = instance\n",
    "\n",
    "        input_ids = self.indexer(tokens_a + tokens_b)\n",
    "        segment_ids = [0]*len(tokens_a) + [1]*len(tokens_b) # token type ids\n",
    "        input_mask = [1]*(len(tokens_a) + len(tokens_b))\n",
    "\n",
    "        label_id = self.label_map[label]\n",
    "\n",
    "        # zero padding\n",
    "        n_pad = self.max_len - len(input_ids)\n",
    "        input_ids.extend([0]*n_pad)\n",
    "        segment_ids.extend([0]*n_pad)\n",
    "        input_mask.extend([0]*n_pad)\n",
    "\n",
    "        return (input_ids, segment_ids, input_mask, label_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2a268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\" Classifier with Transformer \"\"\"\n",
    "    def __init__(self, cfg, n_labels):\n",
    "        super().__init__()\n",
    "        self.transformer = models.Transformer(cfg)\n",
    "        self.fc = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
    "        self.classifier = nn.Linear(cfg.dim, n_labels)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        h = self.transformer(input_ids, segment_ids, input_mask)\n",
    "        # only use the first h in the sequence\n",
    "        pooled_h = self.activ(self.fc(h[:, 0]))\n",
    "        logits = self.classifier(self.drop(pooled_h))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87242d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = \"config/train_mrpc.json\"\n",
    "model_cfg = \"config/bert_base.json\"\n",
    "vocab = \"/home/shariff/layers/pytorchic-bert-2/uncased_L-2_H-128_A-2/vocab.txt\"\n",
    "mode = \"eval\"\n",
    "task = \"mrpc\"\n",
    "data_file = \"/home/shariff/glue_data/MRPC/dev.tsv\"\n",
    "model_file = \"/data/shariff/bert_tiny/output_leaky_210/model_steps_300.pt\"\n",
    "max_len = 128\n",
    "save_dir = \"../exp/bert/mrpc\"\n",
    "data_parallel=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385f8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_class(task):\n",
    "    \"\"\" Mapping from task string to Dataset Class \"\"\"\n",
    "    table = {'mrpc': MRPC, 'mnli': MNLI}\n",
    "    return table[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f62df3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    }
   ],
   "source": [
    "cfg = train.Config.from_json(train_cfg)\n",
    "model_cfg = models.Config.from_json(model_cfg)\n",
    "\n",
    "set_seeds(cfg.seed)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab, do_lower_case=True)\n",
    "TaskDataset = dataset_class(task) # task dataset class according to the task\n",
    "pipeline = [Tokenizing(tokenizer.convert_to_unicode, tokenizer.tokenize),\n",
    "            AddSpecialTokensWithTruncation(max_len),\n",
    "            TokenIndexing(tokenizer.convert_tokens_to_ids,\n",
    "                          TaskDataset.labels, max_len)]\n",
    "dataset = TaskDataset(data_file, pipeline)\n",
    "data_iter = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "model = Classifier(model_cfg, len(TaskDataset.labels))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = train.Trainer(cfg,\n",
    "                        model,\n",
    "                        data_iter,\n",
    "                        optim.optim4GPU(cfg, model),\n",
    "                        save_dir, get_device())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4be4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from /data/shariff/bert_tiny/output_leaky_210/model_steps_300.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.833): 100%|██████████| 13/13 [00:00<00:00, 22.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7156863212585449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == 'train':\n",
    "    def get_loss(model, batch, global_step): # make sure loss is a scalar tensor\n",
    "        input_ids, segment_ids, input_mask, label_id = batch\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "        loss = criterion(logits, label_id)\n",
    "        return loss\n",
    "\n",
    "    trainer.train(get_loss, model_file, pretrain_file, data_parallel)\n",
    "\n",
    "elif mode == 'eval':\n",
    "    def evaluate(model, batch):\n",
    "        input_ids, segment_ids, input_mask, label_id = batch\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "        _, label_pred = logits.max(1)\n",
    "        result = (label_pred == label_id).float() #.cpu().numpy()\n",
    "        accuracy = result.mean()\n",
    "        return accuracy, result\n",
    "\n",
    "    results = trainer.eval(evaluate, model_file, data_parallel)\n",
    "    total_accuracy = torch.cat(results).mean().item()\n",
    "    print('Accuracy:', total_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a3fa2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from /data/shariff/bert_tiny/output_leaky_210/model_steps_300.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.load(model_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac565b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model.to(trainer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406371ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (transformer): Transformer(\n",
       "    (embed): Embeddings(\n",
       "      (tok_embed): Embedding(30522, 128)\n",
       "      (pos_embed): Embedding(512, 128)\n",
       "      (seg_embed): Embedding(2, 128)\n",
       "      (norm): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): BlockLeaky(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForwardLeaky(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): BlockIdentity(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForwardIdentity(\n",
       "          (fc1): Identity()\n",
       "          (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd672530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardIdentity(nn.Module):\n",
    "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        #self.activ = lambda x: activ_fn(cfg.activ_fn, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
    "        activ_fn = nn.Identity()\n",
    "        return self.fc2(activ_fn(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa688d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwff_new = PositionWiseFeedForwardIdentity(cfg)\n",
    "pwff_new = copy.deepcopy(model.transformer.blocks[0].pwff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b36499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = pwff_new._modules['fc1'].weight.data\n",
    "w3 = pwff_new._modules['fc2'].weight.data\n",
    "b0 = pwff_new._modules['fc1'].bias.data\n",
    "b3 = pwff_new._modules['fc2'].bias.data\n",
    "pwff_new._modules['fc2'].weight.data = torch.matmul(w3, w0)\n",
    "pwff_new._modules['fc2'].bias.data = torch.matmul(w3, b0) + b3\n",
    "pwff_new._modules['fc1'] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18831c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a4fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model.transformer.blocks[0].pwff = copy.deepcopy(pwff_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aa3ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (transformer): Transformer(\n",
      "    (embed): Embeddings(\n",
      "      (tok_embed): Embedding(30522, 128)\n",
      "      (pos_embed): Embedding(512, 128)\n",
      "      (seg_embed): Embedding(2, 128)\n",
      "      (norm): LayerNorm()\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): BlockLeaky(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm()\n",
      "        (pwff): PositionWiseFeedForwardLeaky(\n",
      "          (fc1): Identity()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): BlockIdentity(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm()\n",
      "        (pwff): PositionWiseFeedForwardIdentity(\n",
      "          (fc1): Identity()\n",
      "          (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm()\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (activ): Tanh()\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(compressed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78b599be",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = copy.deepcopy(compressed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37f04318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0 = model.transformer.blocks[11].pwff._modules['fc1'].weight.data\n",
    "# w3 = model.transformer.blocks[11].pwff._modules['fc2'].weight.data\n",
    "# b0 = model.transformer.blocks[11].pwff._modules['fc1'].bias.data\n",
    "# b3 = model.transformer.blocks[11].pwff._modules['fc2'].bias.data\n",
    "# model.transformer.blocks[11].pwff._modules['fc2'].weight.data = torch.matmul(w3, w0)\n",
    "# model.transformer.blocks[11].pwff._modules['fc2'].bias.data = torch.matmul(w3, b0) + b3\n",
    "# model.transformer.blocks[11].pwff._modules['fc1'] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc9561a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collapse_layers_bert(model, relu_id=11):\n",
    "#     w0 = model.classifier._modules[str(relu_id - 1)].weight.data\n",
    "#     w3 = model.classifier._modules[str(relu_id + 2)].weight.data\n",
    "#     b0 = model.classifier._modules[str(relu_id - 1)].bias.data\n",
    "#     b3 = model.classifier._modules[str(relu_id + 2)].bias.data\n",
    "#     model.classifier._modules[str(relu_id + 2)].weight.data = torch.matmul(w3, w0)\n",
    "#     model.classifier._modules[str(relu_id + 2)].bias.data = torch.matmul(w3, b0) + b3\n",
    "\n",
    "#     model.classifier._modules[str(relu_id - 1)] = nn.Identity()\n",
    "#     model.classifier._modules[str(relu_id)] = nn.Identity()\n",
    "#     model.classifier._modules[str(relu_id + 1)] = nn.Identity()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e3e081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (transformer): Transformer(\n",
       "    (embed): Embeddings(\n",
       "      (tok_embed): Embedding(30522, 128)\n",
       "      (pos_embed): Embedding(512, 128)\n",
       "      (seg_embed): Embedding(2, 128)\n",
       "      (norm): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): BlockLeaky(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForwardLeaky(\n",
       "          (fc1): Identity()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): BlockIdentity(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForwardIdentity(\n",
       "          (fc1): Identity()\n",
       "          (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5814039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compressed(compressed_model, batch):\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = compressed_model(input_ids, segment_ids, input_mask)\n",
    "    # print(compressed_model)\n",
    "    _, label_pred = logits.max(1)\n",
    "    result = (label_pred == label_id).float() #.cpu().numpy()\n",
    "    accuracy = result.mean()\n",
    "    return accuracy, result\n",
    "\n",
    "# results = trainer.eval(evaluate_compressed, model_file, data_parallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d57c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|██████████| 13/13 [00:00<00:00, 314.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7156863212585449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "iter_bar = tqdm(trainer.data_iter, desc='Iter (loss=X.XXX)')\n",
    "for batch in iter_bar:\n",
    "    batch = [t.to(trainer.device) for t in batch]\n",
    "    with torch.no_grad(): # evaluation without gradient calculation\n",
    "        accuracy, result = evaluate_compressed(compressed_model, batch) # accuracy to print\n",
    "    results.append(result)\n",
    "\n",
    "    iter_bar.set_description('Iter(acc=%5.3f)'%accuracy)\n",
    "\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print('Accuracy:', total_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5615b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4386178\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "768b71c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4270978\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in compressed_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d15160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4155778\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in compressed_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c345ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 16.292MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bd2daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed model size: 15.853MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in compressed_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in compressed_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('compressed model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73966aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(compressed_model.state_dict(), \"/data/shariff/bert_tiny/output_leaky_210/compressed_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f245097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (transformer): Transformer(\n",
       "    (embed): Embeddings(\n",
       "      (tok_embed): Embedding(30522, 128)\n",
       "      (pos_embed): Embedding(512, 128)\n",
       "      (seg_embed): Embedding(2, 128)\n",
       "      (norm): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForward(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): BlockLeaky(\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (proj_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm()\n",
       "        (pwff): PositionWiseFeedForwardLeaky(\n",
       "          (fc1): Identity()\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b64069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
