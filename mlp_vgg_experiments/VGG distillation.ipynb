{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5992899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!export CUDA_VISIBLE_DEVICES=1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f12e427",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union, List\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/__init__.py:443\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# The __file__ check only works for Python 3.7 and above.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _C_for_compiled_check\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124m                of the PyTorch repository rather than the C extensions which\u001b[39m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124m                are expected in the `torch._C` namespace. This can occur when\u001b[39m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124m                using the `install` workflow. e.g.\u001b[39m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124m                    $ python setup.py install && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m \n\u001b[1;32m    451\u001b[0m \u001b[38;5;124m                This error can generally be solved using the `develop` workflow\u001b[39m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124m                    $ python setup.py develop && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  # This should succeed\u001b[39m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(_C):\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory."
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchprofile import profile_macs\n",
    "\n",
    "assert torch.cuda.is_available(), \\\n",
    "\"CUDA support is not available.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a6c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/d6/af/cb5ea6d1a9c83e715e29b45a598ebf542729cd216b43f5deefc27657bd38/tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/shariff/.local/lib/python3.8/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
      "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/shariff/.local/lib/python3.8/site-packages (from tensorflow) (4.24.1)\n",
      "Requirement already satisfied: setuptools in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/14/44/8e501aa789e96f64653ff5be1f110174ed4162c44b2a9f372fcd9e08899c/tensorflow_io_gcs_filesystem-0.33.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/shariff/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/shariff/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/shariff/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.30.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/shariff/anaconda3/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/shariff/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/shariff/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/shariff/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/shariff/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/shariff/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/shariff/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shariff/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shariff/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/shariff/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading tensorflow-2.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.33.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, oauthlib, numpy, keras, google-pasta, gast, astunparse, absl-py, requests-oauthlib, opt-einsum, google-auth, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dpu-utils 0.6.1 requires docopt, which is not installed.\n",
      "fairseq 0.10.2 requires dataclasses, which is not installed.\n",
      "gsutil 5.24 requires fasteners>=0.14.1, which is not installed.\n",
      "gsutil 5.24 requires monotonic>=1.4, which is not installed.\n",
      "peft 0.4.0.dev0 requires safetensors, which is not installed.\n",
      "streamlit 1.23.1 requires blinker<2,>=1.0.0, which is not installed.\n",
      "streamlit 1.23.1 requires gitpython!=3.1.19,<4,>=3, which is not installed.\n",
      "transformers 4.31.0 requires safetensors>=0.3.1, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.22.0 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.22.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.22.0 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-pasta-0.2.0 keras-2.13.1 libclang-16.0.6 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load pre-trained VGG16 model as the teacher model\n",
    "teacher_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Create a smaller student model\n",
    "def create_student_model(input_shape, num_classes):\n",
    "    # Define your smaller student model architecture here\n",
    "    # You can use fewer layers and/or smaller layer sizes compared to the teacher model\n",
    "    # Make sure the architecture is capable of handling the same input_shape and num_classes\n",
    "\n",
    "    return student_model\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 1000\n",
    "\n",
    "student_model = create_student_model(input_shape, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compile the student model\n",
    "student_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[CategoricalAccuracy()])\n",
    "\n",
    "# Prepare your ImageNet data (e.g., using tf.keras.preprocessing.image.ImageDataGenerator)\n",
    "\n",
    "# Perform distillation training\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_images, batch_labels in train_data_generator:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get teacher predictions\n",
    "            teacher_predictions = teacher_model(batch_images, training=False)\n",
    "            \n",
    "            # Get student predictions\n",
    "            student_predictions = student_model(batch_images, training=True)\n",
    "            \n",
    "            # Calculate distillation loss (e.g., Mean Squared Error between teacher and student logits)\n",
    "            distillation_loss = # Calculate distillation loss\n",
    "            \n",
    "            # Calculate classification loss for student model\n",
    "            classification_loss = loss_fn(batch_labels, student_predictions)\n",
    "            \n",
    "            # Total loss is a combination of distillation and classification loss\n",
    "            total_loss = distillation_loss + classification_loss\n",
    "        \n",
    "        # Calculate gradients and update student model\n",
    "        gradients = tape.gradient(total_loss, student_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Classification Loss = {classification_loss:.4f}, Distillation Loss = {distillation_loss:.4f}\")\n",
    "\n",
    "# Evaluate the student model\n",
    "evaluation_results = student_model.evaluate(test_data_generator)\n",
    "print(\"Evaluation results:\", evaluation_results)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1f1c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 20:47:40.410103: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 20:47:41.189297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/shariff/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e705e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76268a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
